Existing Approaches:
* Image CLIP and DINOv2: Mask patches to student, predict embeddings to match teach patches (unmasked)
* MoCo: InfoNCE Contrastive Loss over student and teacher losses (fed different augmentations)
* I-JEPA: Given context crop and multiple target crops, embed both, small prediction network predicts targets from context given target location info
* DINOv2: Different crops of image, feed through student and teacher and cls embedding (projected) the same
* DINOv2: Centers teacher outputs (with moving options) and softmax both outputs to prevent mode collapse

Thoughts:
* Predict if two patches in a batch are the same image
* Predict if a patch is beyond image
* Augmentation (slice interpolation, cropping without resizing, dropping slices and time steps, isolating each, etc.)
* Some Losses
* Ablation with non-dimensional embeddings (just overall for the sequence)

Asks:
* Ask why we do student-teacher









# * Everyone uses student-teacher
# * Image BERT and DINOv2: masked image modeling is generally masking patch and matching the teacher (unmasked) patch embedding (with additional head networks)
# * MoCo: Minibatch of images, augmented 2 ways with second fed to teach model, apply infoNCE contrastive loss to the minibatch (should be like representation of the same original image)
# * Embed context patch and series of target patches and use small predictor network to, given target patch location information, predict the embedding of the target patch, mse loss to be similar
# * DINOv2: given two different crops to the student and teacher, push the cls embeddings (with additional head networks) to be similar
# * DINOv2: before two losses, apply centering to teacher embeddings and softmax to both embeddings (per embedding not per batch) to prevent mode collapse

# Ideas:
# * Predict whether two frames are from the same image
# * Swap time step or slice





* CLS prediction
* Masked patch prediction

* Position prediction (with positional embedding masked)
* (4D) position swapping while maintaining teacher CLS embedding
* Predicting which image a masked patch embedding came from
* Predicting whether a masked patch is masked randomly or by being "outside" the image
* Not masking but adversarially permuting patch embeddings but maintaining the CLS embedding
* Next frame/slice prediction (maybe multiple CLS tokens?)
* Dynamic patching sizes
* Dynamic masked patch size
* Adversarial positional embeddings
* Entropy minimization in unused dimensions (make different time steps or slices be similar)
* Patch coherence loss (minimize the distance between masked patches and neighboring patches)